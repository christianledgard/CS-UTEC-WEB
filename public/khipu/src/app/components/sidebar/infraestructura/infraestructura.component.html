<!-- bloc-8 -->
<div class="row bgc-white" id="bloc-8">
  <div class="col-sm-6">
    <h2 class="mg-md tc-outer-space">
      <strong>Khipu</strong>
    </h2>
    <div class="alert alert-success">
      <p>Última actualización: 23 Agosto 2020.<br /></p>
    </div>
    <p class="text-left">
      Khipu es un cluster dedicado a la computación de alto desempeño, en inglés
      High performance Computing (HPC). Está formado por&nbsp;una colección de
      servidores distribuídos, llamados nodos. Cada nodo está conectados a
      través de una interconexión de alta velocidad (Infiniband),&nbsp;y tiene
      CPUs y/o GPUs, memoria y almacenamiento.
      <br />
      <br />Khipu es parte del Centro de Investigación para la Computación
      Sostenible (CICS) de la Universidad de Ingeniería y Tecnología (UTEC).<br />
    </p>
    <div class="alert alert-info">
      <p>
        <strong>Contacto:</strong> &nbsp;<a href="mailto:khipu@utec.edu.pe"
          >khipu@utec.edu.pe</a
        >. <br />Para información de acceso ver&nbsp;<a
          routerLink="/datos/politica"
          >Política de uso</a
        >.
      </p>
    </div>
    <h4 class="tc-outer-space mg-md">
      <strong>Grupos de usuarios</strong>
    </h4>
    <p>
      Existen dos grupos de usuarios gerenciados automáticamente por&nbsp;<a
        routerLink="/datos/guia"
        >Slurm</a
      >: 1) investigación, y 2) educación. <br />Ambos grupos acceden al cluster
      por el nodo líder para enviar trabajos a la fila de ejecución.&nbsp;
    </p>
    <h5 class="mg-md">
      <strong>Grupo Educación</strong>
    </h5>
    <p class="mg-md">
      Permite el uso del cluster para aula o laboratorio por los estudiantes a
      pedido de un instructor(a) registrado(a). <br />
      Este grupo es financiado por la universidad y sus trabajos serán
      procesados bajo las siguientes características:<br />
    </p>
    <ul class="list-unstyled transp list-sp-lg">
      <li>
        <h5 class="mg-clear">
          <span class="ion ion-clipboard"></span>&nbsp;Ocupar un nodo CPU y/o
          GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de
          almacenamiento.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="feather-icon icon-clock"></span>&nbsp;Tiempo máximo de
          ejecución de trabajo 2 horas.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="feather-icon icon-pause"></span> Número de procesos
          concurrentes 2.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="ion ion-network"></span>&nbsp;Cantidad de cores por
          proceso 8.
        </h5>
      </li>
    </ul>
    <h5 class="mg-md">
      <strong>Grupo Investigación</strong>
    </h5>
    <p class="mg-md">
      Este grupo está financiado por UTEC, proyectos de investigación (PI),
      Departamentos y Dirección de Escuela. <br />Los fondos centrales cubren
      costos de infraestructura, operacion y soporte. Los PI y algunas unidades
      y departamentos financian la adquisición de nuevos nodos de procesamiento
      y almacenamiento. <br />Los usuarios de este grupo ejecutan trabajos en
      todos los nodos bajo las siguientes características: <br />
    </p>
    <ul class="list-unstyled transp list-sp-lg">
      <li>
        <h5 class="mg-clear">
          <span class="ion ion-clipboard"></span>&nbsp;Ocupar cualquier nodo CPU
          y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3
          TB de almacenamiento.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="feather-icon icon-clock"></span>&nbsp;Tiempo máximo de
          ejecución de trabajo NaN.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="feather-icon icon-pause"></span> Número de procesos
          concurrentes NaN.
        </h5>
      </li>
      <li>
        <h5 class="mg-clear">
          <span class="ion ion-network"></span>&nbsp;Cantidad de cores por
          proceso NaN.
        </h5>
      </li>
    </ul>
  </div>
  <div class="col-sm-6">
    <h4 class="mg-md">
      <strong>Infraestructura</strong>
    </h4>
    <p>
      Para mayor información sobre programas, ver&nbsp;<a
        routerLink="/datos/software"
        >Software</a
      >.
    </p>
    <img
      src="assets/img/lazyload-ph.png"
      data-src="assets/img/Figures-01.png"
      class="center-block img-responsive mg-sm img-rd-md lazyload"
    />
    <div class="panel">
      <div class="panel-heading bgc-maya-blue">
        <h4 class="mg-clear">
          <strong>nLíder</strong>
        </h4>
      </div>
      <div class="panel-body">
        <p>
          Nodo de acceso al cluster, usado compilar y principalmente para enviar
          trabajos.
        </p>
        <p class="colorhover">
          <strong>Procesadores:</strong> Intel(R) Xeon(R) Gold 6230 CPU @ 2.10
          GHz<br />20 cores por socket, 40 por nodo. <br />
        </p>
        <p class="colorhover">
          <strong>Memoria:</strong>&nbsp;DRAM DDR4-1333 MHz, 128 GB por nodo<br />
        </p>
        <p class="colorhover">
          <strong>Disco local:</strong>&nbsp;480 SSD, total 40 TB HDD<br />
        </p>
        <p class="colorhover">
          <strong>Network:</strong>&nbsp;Infiniband FDR&nbsp;MT4119<br />
        </p>
      </div>
    </div>
    <div class="panel">
      <div class="panel-heading bgc-maya-blue">
        <h4 class="mg-clear">
          <strong>nCPU</strong>
        </h4>
      </div>
      <div class="panel-body">
        <p>Usado para procesamiento, gerenciado automáticamente por Slurm.</p>
        <p class="colorhover">
          <strong>Procesadores:</strong> Intel(R) Xeon(R) Gold 6130 CPU @2.10
          GHz<br />16 cores por socket, 32 por nodo.<br />
        </p>
        <p class="colorhover">
          <strong>Memoria:</strong>&nbsp;DRAM DDR4-1333 MHz, 128 GB por nodo<br />
        </p>
        <p class="colorhover">
          <strong>Disco local:</strong>&nbsp;960 GB SSD<br />
        </p>
        <p class="colorhover">
          <strong>Network:</strong>&nbsp;Infiniband FDR&nbsp;MT4119<br />
        </p>
      </div>
    </div>
    <div class="panel">
      <div class="panel-heading bgc-maya-blue">
        <h4 class="mg-clear">
          <strong>nGPU</strong>
        </h4>
      </div>
      <div class="panel-body">
        <p>
          Usado para procesamiento, gerenciado automáticamente por Slurm.<br />
        </p>
        <p class="colorhover">
          <strong>Procesadores:</strong> Intel(R) Xeon(R) Gold 6230 CPU @2.10
          GHz<br />20 cores por socket, 40 por nodo. <br />
        </p>
        <p class="colorhover">
          <strong></strong> NVIDIA Tesla T4 &nbsp; <br />16 GB GDDR6, PCIe 3.0
          x16 <br />1 GPU por nodo.
        </p>
        <p class="colorhover">
          <strong>Memoria:</strong>&nbsp;DRAM DDR4-1333 MHz, 128 GB por nodo<br />
        </p>
        <p class="colorhover">
          <strong>Disco local:</strong>&nbsp;480 GB SSD<br />
        </p>
        <p class="colorhover">
          <strong>Network:</strong>&nbsp;Infiniband FDR&nbsp;MT4119<br />
        </p>
      </div>
    </div>
  </div>
</div>

<!-- bloc-8 END -->
